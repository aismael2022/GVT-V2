{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn3xRwfgC3Ak3dZ20up68c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aismael2022/GVT-V2/blob/main/GVT-V2_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omJOaIY-ZKd3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import lru_cache\n",
        "from langdetect import detect\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from transformers import pipeline\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from typing import Dict, Any, Union, Optional\n",
        "from spacy.language import Language\n",
        "\n",
        "# Install required packages for Colab\n",
        "!pip install -q langdetect spacy transformers selenium webdriver_manager pandas numpy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Suppress unnecessary warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --------- Model Loading with Cache --------- #\n",
        "#@lru_cache(maxsize=None)\n",
        "# Type alias for clarity (if using transformers)\n",
        "if 'Optional' not in dir():\n",
        "    Optional = Union[Any, None]  # Fallback definition\n",
        "\n",
        "# For transformer models (replace with actual type if needed)\n",
        "TransformerModel = Any\n",
        "\n",
        "def load_models() -> Dict[str, Union[Language, TransformerModel, None]]:\n",
        "    \"\"\"Cache-loaded NLP models with proper typing support.\"\"\"\n",
        "    models: Dict[str, Union[Language, TransformerModel, None]] = {\n",
        "        'spacy': None,\n",
        "        'transformer': None\n",
        "    }\n",
        "\n",
        "    models['spacy'] = load_spacy_model()\n",
        "    return models\n",
        "\n",
        "def load_spacy_model() -> Union[Language, None]:\n",
        "    \"\"\"Load spaCy model with fallback logic.\"\"\"\n",
        "    model_names = ['en_core_web_sm', 'en_core_web_lg']\n",
        "\n",
        "    for model_name in model_names:\n",
        "        try:\n",
        "            print(f\"üîÑ Loading {model_name}...\")\n",
        "            return spacy.load(model_name)\n",
        "        except OSError:\n",
        "            print(f\"‚ö†Ô∏è {model_name} not found, trying to download...\")\n",
        "            try:\n",
        "                !python -m spacy download {model_name}\n",
        "                return spacy.load(model_name)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Download failed: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(\"‚ùå All model load attempts failed\")\n",
        "    return None\n",
        "\n",
        "# Runtime loading\n",
        "models = load_models()\n",
        "nlp = models['spacy']  # type: Union[Language, None]\n",
        "ner_pipeline = models['transformer']  # type: Union[Any, None]\n",
        "\n",
        "# --------- Core Text Processing --------- #\n",
        "def clean_name(name: str) -> str:\n",
        "    \"\"\"Enhanced name cleaner with Unicode support\"\"\"\n",
        "    if not name:\n",
        "        return \"\"\n",
        "\n",
        "    # Preserve letters, spaces, apostrophes, and Unicode chars\n",
        "    name = re.sub(r\"[^\\w\\s'\\u00C0-\\u017F]\", \"\", str(name), flags=re.UNICODE)\n",
        "    # Remove lonely apostrophes but keep valid ones (O'Connor)\n",
        "    name = re.sub(r\"(?<!\\w)'(?!\\w)\", \"\", name)\n",
        "    # Normalize and trim\n",
        "    return re.sub(r\"\\s+\", \" \", name).strip()\n",
        "\n",
        "def is_person(text: str) -> bool:\n",
        "    \"\"\"Multi-layered person detection with safe model access\"\"\"\n",
        "    text = clean_name(text)\n",
        "    if not text or len(text) < 2 or text.isdigit():\n",
        "        return False\n",
        "\n",
        "    # Exclusion patterns\n",
        "    non_person_terms = {\n",
        "        'tv', 'show', 'movie', 'film', 'series', 'season',\n",
        "        'song', 'music', 'award', 'channel', 'network',\n",
        "        'episode', 'live', 'stream', 'premiere', 'finale'\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    if any(term in text_lower for term in non_person_terms):\n",
        "        return False\n",
        "\n",
        "    # SpaCy NER (with null check)\n",
        "    if nlp is not None:  # Explicit check before calling\n",
        "        try:\n",
        "            doc = nlp(text)\n",
        "            if any(ent.label_ in (\"PER\", \"PERSON\") for ent in doc.ents):\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è SpaCy error: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è SpaCy model not available\")\n",
        "\n",
        "    # Transformer NER (existing null check is fine)\n",
        "    if ner_pipeline:\n",
        "        try:\n",
        "            entities = ner_pipeline(text) or []\n",
        "            for ent in entities:\n",
        "                if getattr(ent, \"entity_group\", None) == \"PER\" or \\\n",
        "                   (isinstance(ent, dict) and ent.get(\"entity_group\") == \"PER\"):\n",
        "                    return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Transformer error: {e}\")\n",
        "\n",
        "    # Western name pattern\n",
        "    if (re.fullmatch(r'([A-Z][a-z]+)(?:\\s+[A-Z][a-z]+){0,3}', text) and\n",
        "        not any(w.lower() in {'the', 'and', 'of'} for w in text.split())):\n",
        "        return True\n",
        "\n",
        "    # Initial-based names\n",
        "    if re.fullmatch(r'([A-Z]\\.\\s*)+[A-Z][a-z]+', text):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# --------- Scraping Functions --------- #\n",
        "def configure_driver() -> webdriver.Chrome:\n",
        "    \"\"\"Configure Chrome WebDriver with stealth options\"\"\"\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")  # Run in headless mode on Colab\n",
        "    options.add_argument(\"--no-sandbox\")  # Required for Colab\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")  # Required for Colab\n",
        "    options.add_argument(\"--window-size=900,300\")\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    options.add_experimental_option(\"useAutomationExtension\", False)\n",
        "\n",
        "    # Install ChromeDriver and configure service\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    return webdriver.Chrome(service=service, options=options)\n",
        "\n",
        "def scrape_trends(driver: webdriver.Chrome, timeout: int = 30, max_names: int = 25) -> set:\n",
        "    \"\"\"Core scraping logic with improved element detection\"\"\"\n",
        "    names = set()\n",
        "    start = time.time()\n",
        "\n",
        "    while (time.time() - start) < timeout and len(names) < max_names:\n",
        "        try:\n",
        "            items = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_all_elements_located(\n",
        "                    (By.CSS_SELECTOR, \"div.NJfIwe a.vcW2ic[jsname='thYVgf']\")\n",
        "                )\n",
        "            )\n",
        "            names.update(clean_name(item.text) for item in items if item.text)\n",
        "            time.sleep(random.uniform(2, 4))\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Scraping iteration error: {e}\")\n",
        "            break\n",
        "\n",
        "    return names\n",
        "\n",
        "def extract_search_result_with_tools_click(url: str) -> str:\n",
        "    \"\"\"Extracts search results count from Google search with Tools click\"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")  # Run in headless mode on Colab\n",
        "    chrome_options.add_argument(\"--no-sandbox\")  # Required for Colab\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Required for Colab\n",
        "    chrome_options.add_argument(\"--window-size=1000,400\")\n",
        "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "    driver = None\n",
        "    try:\n",
        "        # Install ChromeDriver and configure service\n",
        "        service = Service(ChromeDriverManager().install())\n",
        "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "        print(f\"üåê Opening: {url}\")\n",
        "        driver.get(url)\n",
        "        wait = WebDriverWait(driver, 15)\n",
        "\n",
        "        # Click Tools button\n",
        "        tools_button = wait.until(\n",
        "            EC.element_to_be_clickable(\n",
        "                (By.XPATH, \"//div[text()='Tools' or text()='ÿßŸÑÿ£ÿØŸàÿßÿ™']\")\n",
        "            )\n",
        "        )\n",
        "        tools_button.click()\n",
        "        time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        # Get result stats\n",
        "        result_stats = wait.until(\n",
        "            EC.presence_of_element_located((By.ID, \"result-stats\"))\n",
        "        )\n",
        "        raw_text = result_stats.text\n",
        "\n",
        "        # Extract number with regex\n",
        "        match = re.search(r'About ([\\d,]+)', raw_text)\n",
        "        if match:\n",
        "            return match.group(1).replace(\",\", \"\")\n",
        "        return \"0\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Search error on {url}: {str(e)[:100]}...\")  # Truncate long error messages\n",
        "        return \"N/A\"\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "def stage_one_extract_and_save() -> str:\n",
        "    \"\"\"Main scraping function with better error handling\"\"\"\n",
        "    driver = None\n",
        "    try:\n",
        "        driver = configure_driver()\n",
        "        driver.get(\"https://trends.google.com/tv/?geo=US&rows=5&cols=5\")\n",
        "        print(\"üîÑ Processing...\")\n",
        "\n",
        "        names = scrape_trends(driver)\n",
        "        if not names:\n",
        "            print(\"‚ùå No names collected\")\n",
        "            return \"\"\n",
        "\n",
        "        # Process and save data\n",
        "        data = []\n",
        "        for name in names:\n",
        "            try:\n",
        "                lang = detect(name) if len(name) > 3 else \"und\"\n",
        "                data.append({\n",
        "                    \"Name\": name,\n",
        "                    \"Is Person\": is_person(name),\n",
        "                    \"Link\": f\"https://www.google.com/search?q={'+'.join(name.split())}&hl=en&gl=us\"\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Name processing error: {e}\")\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        timestamp = datetime.now().strftime(\"%d_%m_%Y_%I-%M%p\")\n",
        "        filename = f\"google_trends_tv_results_{timestamp}.xlsx\"\n",
        "        df.to_excel(filename, index=False)\n",
        "        print(f\"‚úÖ Saved results to {filename}\")\n",
        "        return filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Fatal scraping error: {e}\")\n",
        "        return \"\"\n",
        "    finally:\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "\n",
        "# --------- Search Enrichment --------- #\n",
        "def stage_two_enrich_search_results(filename: str) -> None:\n",
        "    \"\"\"Enhanced search enrichment with better error handling\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"‚ùå File not found: {filename}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(filename)\n",
        "\n",
        "        # Initialize Search Results column if needed\n",
        "        if \"Search Results\" not in df.columns:\n",
        "            df[\"Search Results\"] = np.nan\n",
        "\n",
        "        total = len(df)\n",
        "        for count, (idx, row) in enumerate(df.iterrows(), 1):\n",
        "            # Skip already processed rows\n",
        "            if pd.notna(row[\"Search Results\"]) and row[\"Search Results\"] not in [\"\", \"N/A\"]:\n",
        "                print(f\"‚úÖ Row {count}/{total}: Already processed\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nüîç Processing {count}/{total}: {row['Name']}\")\n",
        "\n",
        "            # Skip non-person entries\n",
        "            if not row['Is Person']:\n",
        "                df.at[idx, \"Search Results\"] = \"Not a person - skipped\"\n",
        "                df.to_excel(filename, index=False)\n",
        "                continue\n",
        "\n",
        "            # Process search results\n",
        "            result = extract_search_result_with_tools_click(str(row['Link']))\n",
        "            df.at[idx, \"Search Results\"] = result\n",
        "            df.to_excel(filename, index=False)\n",
        "\n",
        "            # Random delay between requests\n",
        "            sleep_time = random.randint(5, 15)\n",
        "            print(f\"‚è≥ Sleeping {sleep_time} seconds...\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        print(\"\\n‚úÖ Enrichment complete!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Enrichment failed: {e}\")\n",
        "\n",
        "# --------- Main Execution --------- #\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üåê Loading Google Trends TV...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        results_file = stage_one_extract_and_save()\n",
        "        if results_file:\n",
        "            print(f\"\\nüîÑ Getting Google Search Results...\")\n",
        "            time.sleep(10)\n",
        "            stage_two_enrich_search_results(results_file)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Process interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Critical failure: {e}\")\n",
        "    finally:\n",
        "        print(f\"\\n‚è±Ô∏è Total runtime: {time.time()-start_time:.2f}s\")"
      ]
    }
  ]
}